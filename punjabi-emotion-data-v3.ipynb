{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2022-11-27T12:41:05.412848Z","iopub.execute_input":"2022-11-27T12:41:05.414178Z","iopub.status.idle":"2022-11-27T12:41:05.432136Z","shell.execute_reply.started":"2022-11-27T12:41:05.414134Z","shell.execute_reply":"2022-11-27T12:41:05.431241Z"},"trusted":true},"execution_count":2,"outputs":[{"name":"stdout","text":"/kaggle/input/punjabi-emotion-data/validation_data.csv\n/kaggle/input/punjabi-emotion-data/training_data.csv\n/kaggle/input/punjabi-emotion-data/test_data.csv\n","output_type":"stream"}]},{"cell_type":"code","source":"!pip install transformers\n!pip install datasets\n!pip install sentencepiece","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:41:05.433829Z","iopub.execute_input":"2022-11-27T12:41:05.434259Z","iopub.status.idle":"2022-11-27T12:41:34.958986Z","shell.execute_reply.started":"2022-11-27T12:41:05.434225Z","shell.execute_reply":"2022-11-27T12:41:34.957823Z"},"trusted":true},"execution_count":3,"outputs":[{"name":"stdout","text":"Requirement already satisfied: transformers in /opt/conda/lib/python3.7/site-packages (4.20.1)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from transformers) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (6.0)\nRequirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.8.1)\nRequirement already satisfied: tqdm>=4.27 in /opt/conda/lib/python3.7/site-packages (from transformers) (4.64.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from transformers) (4.12.0)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (1.21.6)\nRequirement already satisfied: packaging>=20.0 in /opt/conda/lib/python3.7/site-packages (from transformers) (21.3)\nRequirement already satisfied: regex!=2019.12.17 in /opt/conda/lib/python3.7/site-packages (from transformers) (2021.11.10)\nRequirement already satisfied: requests in /opt/conda/lib/python3.7/site-packages (from transformers) (2.28.1)\nRequirement already satisfied: tokenizers!=0.11.3,<0.13,>=0.11.1 in /opt/conda/lib/python3.7/site-packages (from transformers) (0.12.1)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0,>=0.1.0->transformers) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging>=20.0->transformers) (3.0.9)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->transformers) (3.8.0)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (3.3)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2022.6.15)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (1.26.12)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests->transformers) (2.1.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: datasets in /opt/conda/lib/python3.7/site-packages (2.1.0)\nRequirement already satisfied: dill in /opt/conda/lib/python3.7/site-packages (from datasets) (0.3.5.1)\nRequirement already satisfied: pyarrow>=5.0.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (5.0.0)\nRequirement already satisfied: tqdm>=4.62.1 in /opt/conda/lib/python3.7/site-packages (from datasets) (4.64.0)\nRequirement already satisfied: requests>=2.19.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2.28.1)\nRequirement already satisfied: pandas in /opt/conda/lib/python3.7/site-packages (from datasets) (1.3.5)\nRequirement already satisfied: xxhash in /opt/conda/lib/python3.7/site-packages (from datasets) (3.0.0)\nRequirement already satisfied: fsspec[http]>=2021.05.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (2022.7.1)\nRequirement already satisfied: aiohttp in /opt/conda/lib/python3.7/site-packages (from datasets) (3.8.1)\nRequirement already satisfied: numpy>=1.17 in /opt/conda/lib/python3.7/site-packages (from datasets) (1.21.6)\nRequirement already satisfied: huggingface-hub<1.0.0,>=0.1.0 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.8.1)\nRequirement already satisfied: responses<0.19 in /opt/conda/lib/python3.7/site-packages (from datasets) (0.18.0)\nRequirement already satisfied: importlib-metadata in /opt/conda/lib/python3.7/site-packages (from datasets) (4.12.0)\nRequirement already satisfied: packaging in /opt/conda/lib/python3.7/site-packages (from datasets) (21.3)\nRequirement already satisfied: multiprocess in /opt/conda/lib/python3.7/site-packages (from datasets) (0.70.13)\nRequirement already satisfied: filelock in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (3.7.1)\nRequirement already satisfied: pyyaml>=5.1 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (6.0)\nRequirement already satisfied: typing-extensions>=3.7.4.3 in /opt/conda/lib/python3.7/site-packages (from huggingface-hub<1.0.0,>=0.1.0->datasets) (4.3.0)\nRequirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /opt/conda/lib/python3.7/site-packages (from packaging->datasets) (3.0.9)\nRequirement already satisfied: charset-normalizer<3,>=2 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2.1.0)\nRequirement already satisfied: urllib3<1.27,>=1.21.1 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (1.26.12)\nRequirement already satisfied: certifi>=2017.4.17 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (2022.6.15)\nRequirement already satisfied: idna<4,>=2.5 in /opt/conda/lib/python3.7/site-packages (from requests>=2.19.0->datasets) (3.3)\nRequirement already satisfied: attrs>=17.3.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (21.4.0)\nRequirement already satisfied: asynctest==0.13.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (0.13.0)\nRequirement already satisfied: multidict<7.0,>=4.5 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (6.0.2)\nRequirement already satisfied: yarl<2.0,>=1.0 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.7.2)\nRequirement already satisfied: frozenlist>=1.1.1 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.3.0)\nRequirement already satisfied: aiosignal>=1.1.2 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (1.2.0)\nRequirement already satisfied: async-timeout<5.0,>=4.0.0a3 in /opt/conda/lib/python3.7/site-packages (from aiohttp->datasets) (4.0.2)\nRequirement already satisfied: zipp>=0.5 in /opt/conda/lib/python3.7/site-packages (from importlib-metadata->datasets) (3.8.0)\nRequirement already satisfied: python-dateutil>=2.7.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2.8.2)\nRequirement already satisfied: pytz>=2017.3 in /opt/conda/lib/python3.7/site-packages (from pandas->datasets) (2022.1)\nRequirement already satisfied: six>=1.5 in /opt/conda/lib/python3.7/site-packages (from python-dateutil>=2.7.3->pandas->datasets) (1.15.0)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0mRequirement already satisfied: sentencepiece in /opt/conda/lib/python3.7/site-packages (0.1.97)\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"from datasets import load_dataset\ndataset = load_dataset('emotion')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:41:34.961100Z","iopub.execute_input":"2022-11-27T12:41:34.961509Z","iopub.status.idle":"2022-11-27T12:42:05.485940Z","shell.execute_reply.started":"2022-11-27T12:41:34.961461Z","shell.execute_reply":"2022-11-27T12:42:05.484969Z"},"trusted":true},"execution_count":4,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading builder script:   0%|          | 0.00/1.66k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"4eff43b1aa2d471295a89b9b27e8544e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading metadata:   0%|          | 0.00/1.61k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"196ded023e334be98d3167121a7a2750"}},"metadata":{}},{"name":"stdout","text":"Downloading and preparing dataset emotion/default (download: 1.97 MiB, generated: 2.07 MiB, post-processed: Unknown size, total: 4.05 MiB) to /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/1.66M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5f756981aa104b3db51162c5a3fa741d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/204k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"38527e0ec1b145dfb00edbb8ea03de9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading data:   0%|          | 0.00/207k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6a688f04f59044d9b1d4161ae87dac35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating train split:   0%|          | 0/16000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating validation split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Generating test split:   0%|          | 0/2000 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stdout","text":"Dataset emotion downloaded and prepared to /root/.cache/huggingface/datasets/emotion/default/0.0.0/348f63ca8e27b3713b6c04d723efe6d824a56fb3d1449794716c0f0296072705. Subsequent calls will reuse this data.\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"873a7eeb3f0348f0bf91fbe459de3544"}},"metadata":{}}]},{"cell_type":"code","source":"\n# Importing the libraries needed\nimport pandas as pd\nimport numpy as np\nfrom sklearn.model_selection import train_test_split\nimport torch\nimport seaborn as sns\nimport transformers\nimport json\nfrom tqdm import tqdm\nfrom torch.utils.data import Dataset, DataLoader\nfrom transformers import RobertaModel, RobertaTokenizer\nimport logging\nlogging.basicConfig(level=logging.ERROR)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:05.488668Z","iopub.execute_input":"2022-11-27T12:42:05.489489Z","iopub.status.idle":"2022-11-27T12:42:08.576914Z","shell.execute_reply.started":"2022-11-27T12:42:05.489441Z","shell.execute_reply":"2022-11-27T12:42:08.575763Z"},"trusted":true},"execution_count":5,"outputs":[]},{"cell_type":"code","source":"# Setting up the device for GPU usage\n\nfrom torch import cuda\ndevice = 'cuda' if cuda.is_available() else 'cpu'\ndevice","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:08.578977Z","iopub.execute_input":"2022-11-27T12:42:08.579391Z","iopub.status.idle":"2022-11-27T12:42:08.638607Z","shell.execute_reply.started":"2022-11-27T12:42:08.579353Z","shell.execute_reply":"2022-11-27T12:42:08.637706Z"},"trusted":true},"execution_count":6,"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"'cuda'"},"metadata":{}}]},{"cell_type":"code","source":"train_dataset = pd.read_csv('/kaggle/input/punjabi-emotion-data/training_data.csv')\ntrain_dataset.head()\n\n\n# train_dataset = dataset['train']\ntext_train=[]\nlabel_train=[]\nfor i in range(len(train_dataset)):\n    text_train.append(train_dataset.iloc[i]['punjabi_emotion'])\n    label_train.append(train_dataset.iloc[i]['labels'])\nlen(text_train),len(label_train)\n\ntrain_df = pd.DataFrame(list(zip(text_train, label_train)),\n               columns =['text','labels'])\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:08.641453Z","iopub.execute_input":"2022-11-27T12:42:08.641755Z","iopub.status.idle":"2022-11-27T12:42:11.969542Z","shell.execute_reply.started":"2022-11-27T12:42:08.641729Z","shell.execute_reply":"2022-11-27T12:42:11.968530Z"},"trusted":true},"execution_count":7,"outputs":[{"execution_count":7,"output_type":"execute_result","data":{"text/plain":"(16000, 2)"},"metadata":{}}]},{"cell_type":"code","source":"\n# test_dataset = dataset['test']\ntest_dataset = pd.read_csv('/kaggle/input/punjabi-emotion-data/test_data.csv')\ntext_test=[]\nlabel_test=[]\nfor i in range(len(test_dataset)):\n    text_test.append(test_dataset.iloc[i]['punjabi_emotion'])\n    label_test.append(test_dataset.iloc[i]['labels'])\nprint(len(text_test),len(label_test))\ntest_df = pd.DataFrame(list(zip(text_test, label_test)),\n               columns =['text','labels'])\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:11.971021Z","iopub.execute_input":"2022-11-27T12:42:11.971370Z","iopub.status.idle":"2022-11-27T12:42:12.410331Z","shell.execute_reply.started":"2022-11-27T12:42:11.971336Z","shell.execute_reply":"2022-11-27T12:42:12.409269Z"},"trusted":true},"execution_count":8,"outputs":[{"name":"stdout","text":"2000 2000\n","output_type":"stream"}]},{"cell_type":"code","source":"\nvalidation_dataset = pd.read_csv('/kaggle/input/punjabi-emotion-data/validation_data.csv')\ntext_validation=[]\nlabel_validation=[]\nfor i in range(len(validation_dataset)):\n    text_validation.append(validation_dataset.iloc[i]['punjabi_emotion'])\n    label_validation.append(validation_dataset.iloc[i]['labels'])\n\nprint(len(text_validation),len(label_validation))\nvalidation_df = pd.DataFrame(list(zip(text_validation, label_validation)),\n               columns =['text','labels'])\n\n# validation.to_csv('english_validation_data.csv')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:12.411941Z","iopub.execute_input":"2022-11-27T12:42:12.412308Z","iopub.status.idle":"2022-11-27T12:42:12.875382Z","shell.execute_reply.started":"2022-11-27T12:42:12.412271Z","shell.execute_reply":"2022-11-27T12:42:12.874294Z"},"trusted":true},"execution_count":9,"outputs":[{"name":"stdout","text":"2000 2000\n","output_type":"stream"}]},{"cell_type":"code","source":"import re\ndef strip_punc(txt):\n    \"\"\"\n    To get only alphabets a-z and A-Z from text\n        :param txt: input text for cleaning [str]\n        :return: text which only contains alphabets a-z A-Z [str]\n    \"\"\"\n    \n    ret = re.findall('[A-Za-z]+',str(txt))\n    if ret:\n        return True\n    else:\n        return False","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:12.876892Z","iopub.execute_input":"2022-11-27T12:42:12.877842Z","iopub.status.idle":"2022-11-27T12:42:12.884144Z","shell.execute_reply.started":"2022-11-27T12:42:12.877804Z","shell.execute_reply":"2022-11-27T12:42:12.882764Z"},"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"text = train_df['text'].tolist()\n\nkll=[]\nfor i in text:\n    if strip_punc(i):\n        g = strip_punc(i)\n        kll.append(g)\n    else:\n        g = strip_punc(i)\n        kll.append(g)\n\n\ntrain_df['wrong_data_flag'] = kll\n\ntrain_df.head(10)\n\ntrain_df = train_df.loc[train_df['wrong_data_flag']!=True]\ntrain_df['wrong_data_flag'].value_counts()","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:12.889061Z","iopub.execute_input":"2022-11-27T12:42:12.889393Z","iopub.status.idle":"2022-11-27T12:42:13.040036Z","shell.execute_reply.started":"2022-11-27T12:42:12.889366Z","shell.execute_reply":"2022-11-27T12:42:13.039001Z"},"trusted":true},"execution_count":11,"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"False    15485\nName: wrong_data_flag, dtype: int64"},"metadata":{}}]},{"cell_type":"code","source":"\n\ntext = validation_df['text'].tolist()\n\nkll=[]\nfor i in text:\n    if strip_punc(i):\n        g = strip_punc(i)\n        kll.append(g)\n    else:\n        g = strip_punc(i)\n        kll.append(g)\n\n\nvalidation_df['wrong_data_flag'] = kll\nvalidation_df = validation_df.loc[validation_df['wrong_data_flag']!=True]\nvalidation_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:13.041436Z","iopub.execute_input":"2022-11-27T12:42:13.042099Z","iopub.status.idle":"2022-11-27T12:42:13.070484Z","shell.execute_reply.started":"2022-11-27T12:42:13.042059Z","shell.execute_reply":"2022-11-27T12:42:13.069674Z"},"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(1936, 3)"},"metadata":{}}]},{"cell_type":"code","source":"\n\ntext = test_df['text'].tolist()\n\nkll=[]\nfor i in text:\n    if strip_punc(i):\n        g = strip_punc(i)\n        kll.append(g)\n    else:\n        g = strip_punc(i)\n        kll.append(g)\n\n\ntest_df['wrong_data_flag'] = kll\ntest_df = test_df.loc[test_df['wrong_data_flag']!=True]\ntest_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:13.072146Z","iopub.execute_input":"2022-11-27T12:42:13.072604Z","iopub.status.idle":"2022-11-27T12:42:13.102190Z","shell.execute_reply.started":"2022-11-27T12:42:13.072551Z","shell.execute_reply":"2022-11-27T12:42:13.101126Z"},"trusted":true},"execution_count":13,"outputs":[{"execution_count":13,"output_type":"execute_result","data":{"text/plain":"(1942, 3)"},"metadata":{}}]},{"cell_type":"code","source":"test_df.reset_index(drop=True,inplace=True)\nvalidation_df.reset_index(drop=True,inplace=True)\ntrain_df.reset_index(drop=True,inplace=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:13.103329Z","iopub.execute_input":"2022-11-27T12:42:13.103771Z","iopub.status.idle":"2022-11-27T12:42:13.109488Z","shell.execute_reply.started":"2022-11-27T12:42:13.103730Z","shell.execute_reply":"2022-11-27T12:42:13.108484Z"},"trusted":true},"execution_count":14,"outputs":[]},{"cell_type":"markdown","source":"<a id='section03'></a>\n### Preparing the Dataset and Dataloader\n\nI will start with defining few key variables that will be used later during the training/fine tuning stage.\nFollowed by creation of Dataset class - This defines how the text is pre-processed before sending it to the neural network. I will also define the Dataloader that will feed  the data in batches to the neural network for suitable training and processing. \nDataset and Dataloader are constructs of the PyTorch library for defining and controlling the data pre-processing and its passage to neural network. For further reading into Dataset and Dataloader read the [docs at PyTorch](https://pytorch.org/docs/stable/data.html)\n\n#### *SentimentData* Dataset Class\n- This class is defined to accept the Dataframe as input and generate tokenized output that is used by the Roberta model for training. \n- I am using the Roberta tokenizer to tokenize the data in the `TITLE` column of the dataframe. \n- The tokenizer uses the `encode_plus` method to perform tokenization and generate the necessary outputs, namely: `ids`, `attention_mask`\n- To read further into the tokenizer, [refer to this document](https://huggingface.co/transformers/model_doc/roberta.html#robertatokenizer)\n- `target` is the encoded category on the news headline. \n- The *SentimentData* class is used to create 2 datasets, for training and for validation.\n- *Training Dataset* is used to fine tune the model: **80% of the original data**\n- *Validation Dataset* is used to evaluate the performance of the model. The model has not seen this data during training. \n\n#### Dataloader\n- Dataloader is used to for creating training and validation dataloader that load data to the neural network in a defined manner. This is needed because all the data from the dataset cannot be loaded to the memory at once, hence the amount of dataloaded to the memory and then passed to the neural network needs to be controlled.\n- This control is achieved using the parameters such as `batch_size` and `max_len`.\n- Training and Validation dataloaders are used in the training and validation part of the flow respectively","metadata":{}},{"cell_type":"code","source":"# Defining some key variables that will be used later on in the training\nMAX_LEN = 128\nTRAIN_BATCH_SIZE = 16\nVALID_BATCH_SIZE = 8\n# EPOCHS = 1\nLEARNING_RATE = 5e-04\n\nfrom transformers import XLMRobertaTokenizer\n\n# Download the tokenizer for the XLM-Robert `base` model.\ntokenizer = XLMRobertaTokenizer.from_pretrained(\"xlm-roberta-base\",truncation=True, do_lower_case=False )\n# tokenizer = RobertaTokenizer.from_pretrained('roberta-base', truncation=True, do_lower_case=True)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:13.111049Z","iopub.execute_input":"2022-11-27T12:42:13.111709Z","iopub.status.idle":"2022-11-27T12:42:22.559582Z","shell.execute_reply.started":"2022-11-27T12:42:13.111672Z","shell.execute_reply":"2022-11-27T12:42:22.558596Z"},"trusted":true},"execution_count":15,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/4.83M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f9cb19bed20a45eaa8a76139f21aea8b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/615 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a4eba1fc47604ec7916b1a3ffe21f8d5"}},"metadata":{}}]},{"cell_type":"code","source":"# new_df = train\ntrain_df.shape","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:22.560917Z","iopub.execute_input":"2022-11-27T12:42:22.561269Z","iopub.status.idle":"2022-11-27T12:42:22.569953Z","shell.execute_reply.started":"2022-11-27T12:42:22.561233Z","shell.execute_reply":"2022-11-27T12:42:22.569035Z"},"trusted":true},"execution_count":16,"outputs":[{"execution_count":16,"output_type":"execute_result","data":{"text/plain":"(15485, 3)"},"metadata":{}}]},{"cell_type":"code","source":"train_df=train_df[['text','labels']]\nvalidation_df = validation_df[['text','labels']]\ntest_df = test_df[['text','labels']]","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:22.572328Z","iopub.execute_input":"2022-11-27T12:42:22.573395Z","iopub.status.idle":"2022-11-27T12:42:22.586125Z","shell.execute_reply.started":"2022-11-27T12:42:22.573358Z","shell.execute_reply":"2022-11-27T12:42:22.585135Z"},"trusted":true},"execution_count":17,"outputs":[]},{"cell_type":"code","source":"class SentimentData(Dataset):\n    def __init__(self, dataframe, tokenizer, max_len):\n        self.tokenizer = tokenizer\n        self.data = dataframe\n        self.text = dataframe.text\n        self.targets = self.data.labels\n        self.max_len = max_len\n\n    def __len__(self):\n        return len(self.text)\n\n    def __getitem__(self, index):\n        text = str(self.text[index])\n        text = \" \".join(text.split())\n\n        inputs = self.tokenizer.encode_plus(\n            text,\n            None,\n            add_special_tokens=True,\n            max_length=self.max_len,\n            pad_to_max_length=True,\n            return_token_type_ids=True,\n            truncation=True\n        )\n        ids = inputs['input_ids']\n        mask = inputs['attention_mask']\n        token_type_ids = inputs[\"token_type_ids\"]\n\n\n        return {\n            'ids': torch.tensor(ids, dtype=torch.long),\n            'mask': torch.tensor(mask, dtype=torch.long),\n            'token_type_ids': torch.tensor(token_type_ids, dtype=torch.long),\n            'targets': torch.tensor(self.targets[index], dtype=torch.float)\n        }","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:22.587595Z","iopub.execute_input":"2022-11-27T12:42:22.588110Z","iopub.status.idle":"2022-11-27T12:42:22.599169Z","shell.execute_reply.started":"2022-11-27T12:42:22.588070Z","shell.execute_reply":"2022-11-27T12:42:22.598233Z"},"trusted":true},"execution_count":18,"outputs":[]},{"cell_type":"code","source":"train_data = train_df\ntest_data = test_df\nvalidation_data = validation_df\n\nprint(\"TRAIN Dataset: {}\".format(train_data.shape))\nprint(\"TEST Dataset: {}\".format(test_data.shape))\nprint(\"Validation Dataset: {}\".format(validation_data.shape))\n\n\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:22.600648Z","iopub.execute_input":"2022-11-27T12:42:22.601133Z","iopub.status.idle":"2022-11-27T12:42:22.610498Z","shell.execute_reply.started":"2022-11-27T12:42:22.601095Z","shell.execute_reply":"2022-11-27T12:42:22.609442Z"},"trusted":true},"execution_count":19,"outputs":[{"name":"stdout","text":"TRAIN Dataset: (15485, 2)\nTEST Dataset: (1942, 2)\nValidation Dataset: (1936, 2)\n","output_type":"stream"}]},{"cell_type":"code","source":"training_set = SentimentData(train_data, tokenizer, MAX_LEN)\nvalidation_set = SentimentData(validation_data,tokenizer,MAX_LEN)\ntesting_set = SentimentData(test_data, tokenizer, MAX_LEN)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:22.612139Z","iopub.execute_input":"2022-11-27T12:42:22.612800Z","iopub.status.idle":"2022-11-27T12:42:22.620383Z","shell.execute_reply.started":"2022-11-27T12:42:22.612745Z","shell.execute_reply":"2022-11-27T12:42:22.619220Z"},"trusted":true},"execution_count":20,"outputs":[]},{"cell_type":"code","source":"train_data.shape,validation_data.shape,test_data.shape,len(training_set)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:22.622347Z","iopub.execute_input":"2022-11-27T12:42:22.622808Z","iopub.status.idle":"2022-11-27T12:42:22.632373Z","shell.execute_reply.started":"2022-11-27T12:42:22.622772Z","shell.execute_reply":"2022-11-27T12:42:22.631132Z"},"trusted":true},"execution_count":21,"outputs":[{"execution_count":21,"output_type":"execute_result","data":{"text/plain":"((15485, 2), (1936, 2), (1942, 2), 15485)"},"metadata":{}}]},{"cell_type":"code","source":"train_params = {'batch_size': TRAIN_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 8\n                }\n\nval_params = {'batch_size': VALID_BATCH_SIZE,\n                'shuffle': True,\n                'num_workers': 8\n                }\n\ntraining_loader = DataLoader(training_set, **train_params)\nvalidation_loader = DataLoader(validation_set, **val_params)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:22.633861Z","iopub.execute_input":"2022-11-27T12:42:22.634283Z","iopub.status.idle":"2022-11-27T12:42:22.642613Z","shell.execute_reply.started":"2022-11-27T12:42:22.634249Z","shell.execute_reply":"2022-11-27T12:42:22.641566Z"},"trusted":true},"execution_count":22,"outputs":[{"name":"stderr","text":"/opt/conda/lib/python3.7/site-packages/torch/utils/data/dataloader.py:490: UserWarning: This DataLoader will create 8 worker processes in total. Our suggested max number of worker in current system is 2, which is smaller than what this DataLoader is going to create. Please be aware that excessive worker creation might get DataLoader running slow or even freeze, lower the worker number to avoid potential slowness/freeze if necessary.\n  cpuset_checked))\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id='section04'></a>\n### Creating the Neural Network for Fine Tuning\n\n#### Neural Network\n - We will be creating a neural network with the `RobertaClass`. \n - This network will have the Roberta Language model followed by a `dropout` and finally a `Linear` layer to obtain the final outputs. \n - The data will be fed to the Roberta Language model as defined in the dataset. \n - Final layer outputs is what will be compared to the `Sentiment category` to determine the accuracy of models prediction. \n - We will initiate an instance of the network called `model`. This instance will be used for training and then to save the final trained model for future inference. \n \n#### Loss Function and Optimizer\n - `Loss Function` and `Optimizer` and defined in the next cell.\n - The `Loss Function` is used the calculate the difference in the output created by the model and the actual output. \n - `Optimizer` is used to update the weights of the neural network to improve its performance.","metadata":{}},{"cell_type":"code","source":"from transformers import XLMRobertaForSequenceClassification\n# model = RobertaClass()\n\n# sadness (0), joy (1), love (2), anger (3), fear (4), surprise (5).\nemotion_map = {0:'sadness',\n               1:'joy', 2:'love',3:'anger',4:'fear',5:'surprise'\n               }\nmodel  = XLMRobertaForSequenceClassification.from_pretrained(\"xlm-roberta-base\", num_labels=len(emotion_map))\nmodel.to(device)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:42:22.644104Z","iopub.execute_input":"2022-11-27T12:42:22.645120Z","iopub.status.idle":"2022-11-27T12:44:10.168722Z","shell.execute_reply.started":"2022-11-27T12:42:22.645083Z","shell.execute_reply":"2022-11-27T12:44:10.167587Z"},"trusted":true},"execution_count":23,"outputs":[{"output_type":"display_data","data":{"text/plain":"Downloading:   0%|          | 0.00/1.04G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"81a46fcdf42246488ee8f0f4e13e8cbb"}},"metadata":{}},{"name":"stderr","text":"Some weights of the model checkpoint at xlm-roberta-base were not used when initializing XLMRobertaForSequenceClassification: ['lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.weight', 'lm_head.layer_norm.weight', 'lm_head.dense.bias', 'roberta.pooler.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias']\n- This IS expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n- This IS NOT expected if you are initializing XLMRobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\nSome weights of XLMRobertaForSequenceClassification were not initialized from the model checkpoint at xlm-roberta-base and are newly initialized: ['classifier.dense.bias', 'classifier.dense.weight', 'classifier.out_proj.bias', 'classifier.out_proj.weight']\nYou should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n","output_type":"stream"},{"execution_count":23,"output_type":"execute_result","data":{"text/plain":"XLMRobertaForSequenceClassification(\n  (roberta): RobertaModel(\n    (embeddings): RobertaEmbeddings(\n      (word_embeddings): Embedding(250002, 768, padding_idx=1)\n      (position_embeddings): Embedding(514, 768, padding_idx=1)\n      (token_type_embeddings): Embedding(1, 768)\n      (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n      (dropout): Dropout(p=0.1, inplace=False)\n    )\n    (encoder): RobertaEncoder(\n      (layer): ModuleList(\n        (0): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (1): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (2): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (3): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (4): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (5): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (6): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (7): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (8): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (9): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (10): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n        (11): RobertaLayer(\n          (attention): RobertaAttention(\n            (self): RobertaSelfAttention(\n              (query): Linear(in_features=768, out_features=768, bias=True)\n              (key): Linear(in_features=768, out_features=768, bias=True)\n              (value): Linear(in_features=768, out_features=768, bias=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n            (output): RobertaSelfOutput(\n              (dense): Linear(in_features=768, out_features=768, bias=True)\n              (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n              (dropout): Dropout(p=0.1, inplace=False)\n            )\n          )\n          (intermediate): RobertaIntermediate(\n            (dense): Linear(in_features=768, out_features=3072, bias=True)\n            (intermediate_act_fn): GELUActivation()\n          )\n          (output): RobertaOutput(\n            (dense): Linear(in_features=3072, out_features=768, bias=True)\n            (LayerNorm): LayerNorm((768,), eps=1e-05, elementwise_affine=True)\n            (dropout): Dropout(p=0.1, inplace=False)\n          )\n        )\n      )\n    )\n  )\n  (classifier): RobertaClassificationHead(\n    (dense): Linear(in_features=768, out_features=768, bias=True)\n    (dropout): Dropout(p=0.1, inplace=False)\n    (out_proj): Linear(in_features=768, out_features=6, bias=True)\n  )\n)"},"metadata":{}}]},{"cell_type":"markdown","source":"<a id='section05'></a>\n### Fine Tuning the Model\n\nAfter all the effort of loading and preparing the data and datasets, creating the model and defining its loss and optimizer. This is probably the easier steps in the process. \n\nHere we define a training function that trains the model on the training dataset created above, specified number of times (EPOCH), An epoch defines how many times the complete data will be passed through the network. \n\nFollowing events happen in this function to fine tune the neural network:\n- The dataloader passes data to the model based on the batch size. \n- Subsequent output from the model and the actual category are compared to calculate the loss. \n- Loss value is used to optimize the weights of the neurons in the network.\n- After every 5000 steps the loss value is printed in the console.\n\nAs you can see just in 1 epoch by the final step the model was working with a loss of 0.8141926634122427.","metadata":{}},{"cell_type":"code","source":"!pip install chime","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:44:10.170124Z","iopub.execute_input":"2022-11-27T12:44:10.170749Z","iopub.status.idle":"2022-11-27T12:44:22.711803Z","shell.execute_reply.started":"2022-11-27T12:44:10.170709Z","shell.execute_reply":"2022-11-27T12:44:22.710640Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"Collecting chime\n  Downloading chime-0.5.3-py3-none-any.whl (3.0 MB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.0/3.0 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n\u001b[?25hInstalling collected packages: chime\nSuccessfully installed chime-0.5.3\n\u001b[33mWARNING: Running pip as the 'root' user can result in broken permissions and conflicting behaviour with the system package manager. It is recommended to use a virtual environment instead: https://pip.pypa.io/warnings/venv\u001b[0m\u001b[33m\n\u001b[0m","output_type":"stream"}]},{"cell_type":"code","source":"# Creating the loss function and optimizer\nloss_function = torch.nn.CrossEntropyLoss()\n# optimizer = torch.optim.Adam(params =  model.parameters(), lr=LEARNING_RATE)\n\noptimizer = torch.optim.AdamW(model.parameters(),\n                  lr = 5e-6, # args.learning_rate\n                  eps = 1e-8 # args.adam_epsilon  - default is 1e-8.\n                )\nfrom sklearn.metrics import f1_score","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:44:22.713680Z","iopub.execute_input":"2022-11-27T12:44:22.714084Z","iopub.status.idle":"2022-11-27T12:44:22.721915Z","shell.execute_reply.started":"2022-11-27T12:44:22.714044Z","shell.execute_reply":"2022-11-27T12:44:22.720887Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"markdown","source":"## Traning Function","metadata":{}},{"cell_type":"code","source":"# Defining the training function on the 80% of the dataset for tuning the distilbert model\nimport chime\ndef train(epoch):\n    tr_loss = 0\n    n_correct = 0\n    nb_tr_steps = 0\n    nb_tr_examples = 0\n    model.train()\n    targets_list=[]\n    preds_list =[]\n    chime.success()\n    for _,data in tqdm(enumerate(training_loader, 0)):\n        try:\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype = torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n\n            outputs = model(ids, mask, token_type_ids)\n\n            # print(outputs,\"\\n\",outputs.logits)\n\n            loss = loss_function(outputs.logits, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.logits, dim=1)\n\n          # to comment\n            # n_correct += calcuate_accuracy(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n\n            targets_list.extend(list(targets.cpu().detach().numpy()))\n            preds_list.extend(list(big_idx.cpu().detach().numpy()))\n\n            # if _%2000==0:\n            #     loss_step = tr_loss/nb_tr_steps\n            #     accu_step = (n_correct*100)/nb_tr_examples \n            #     print(f\"Training Loss per 2000 steps: {loss_step}\")\n            #     print(f\"Training Accuracy per 2000 steps: {accu_step}\")\n\n            optimizer.zero_grad()\n            loss.backward()\n            # # When using GPU\n            optimizer.step()\n        except:\n            continue\n\n    print(len(preds_list),len(targets_list))\n    f1_sc = f1_score(targets_list,preds_list,average='micro')\n\n    # print(f'The Total Accuracy for Epoch {epoch}: {(n_correct*100)/nb_tr_examples}')\n    print(f'F1 score on Epoch {epoch} : {f1_sc}')\n    # epoch_loss = tr_loss/nb_tr_steps\n    # epoch_accu = (n_correct*100)/nb_tr_examples\n    # print(f\"Training Loss Epoch: {epoch_loss}\")\n    # print(f\"Training Accuracy Epoch: {epoch_accu}\")\n\n    return f1_sc","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:44:22.723230Z","iopub.execute_input":"2022-11-27T12:44:22.723605Z","iopub.status.idle":"2022-11-27T12:44:22.742419Z","shell.execute_reply.started":"2022-11-27T12:44:22.723553Z","shell.execute_reply":"2022-11-27T12:44:22.741441Z"},"trusted":true},"execution_count":26,"outputs":[]},{"cell_type":"code","source":"import warnings\nwarnings.filterwarnings('ignore')\nEPOCHS = 15\nfor epoch in range(EPOCHS):\n    train(epoch)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:44:22.744010Z","iopub.execute_input":"2022-11-27T12:44:22.744361Z","iopub.status.idle":"2022-11-27T13:41:16.410792Z","shell.execute_reply.started":"2022-11-27T12:44:22.744327Z","shell.execute_reply":"2022-11-27T13:41:16.409638Z"},"trusted":true},"execution_count":27,"outputs":[{"name":"stderr","text":"968it [03:48,  4.24it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 0 : 0.43797223119147555\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 1 : 0.6093639005489183\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 2 : 0.7441394898288666\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 3 : 0.7960607039070068\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 4 : 0.8170487568614788\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 5 : 0.8335808847271553\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 6 : 0.8498546980949304\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 7 : 0.8605101711333548\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 8 : 0.8733613174039393\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 9 : 0.8810461737164998\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 10 : 0.8909912818856959\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 11 : 0.8967387794639974\n","output_type":"stream"},{"name":"stderr","text":"968it [03:47,  4.26it/s]","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 12 : 0.904875686147885\n","output_type":"stream"},{"name":"stderr","text":"\n968it [03:47,  4.26it/s]\n","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 13 : 0.9139812721989021\n","output_type":"stream"},{"name":"stderr","text":"968it [03:46,  4.27it/s]","output_type":"stream"},{"name":"stdout","text":"15485 15485\nF1 score on Epoch 14 : 0.9186309331611237\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"<a id='section06'></a>\n### Validating the Model\n\nDuring the validation stage we pass the unseen data(Testing Dataset) to the model. This step determines how good the model performs on the unseen data. \n\nThis unseen data is the 20% of `train.tsv` which was seperated during the Dataset creation stage. \nDuring the validation stage the weights of the model are not updated. Only the final output is compared to the actual value. This comparison is then used to calcuate the accuracy of the model. \n\nAs you can see the model is predicting the correct category of a given sample to a 69.47% accuracy which can further be improved by training more.","metadata":{}},{"cell_type":"code","source":"def valid(model, testing_loader):\n    model.eval()\n    n_correct = 0; n_wrong = 0; total = 0; tr_loss=0; nb_tr_steps=0; nb_tr_examples=0\n    targets_list=[]\n    preds_list =[]\n    with torch.no_grad():\n        for _, data in tqdm(enumerate(validation_loader, 0)):\n            ids = data['ids'].to(device, dtype = torch.long)\n            mask = data['mask'].to(device, dtype = torch.long)\n            token_type_ids = data['token_type_ids'].to(device, dtype=torch.long)\n            targets = data['targets'].to(device, dtype = torch.long)\n            outputs = model(ids, mask, token_type_ids)\n            loss = loss_function(outputs.logits, targets)\n            tr_loss += loss.item()\n            big_val, big_idx = torch.max(outputs.logits, dim=1)\n\n            # n_correct += calcuate_accuracy(big_idx, targets)\n\n            nb_tr_steps += 1\n            nb_tr_examples+=targets.size(0)\n\n            targets_list.extend(list(targets.cpu().detach().numpy()))\n            preds_list.extend(list(big_idx.cpu().detach().numpy()))\n            \n            # if _%5000==0:\n            #     loss_step = tr_loss/nb_tr_steps\n            #     accu_step = (n_correct*100)/nb_tr_examples\n            #     print(f\"Validation Loss per 100 steps: {loss_step}\")\n            #     print(f\"Validation Accuracy per 100 steps: {accu_step}\")\n\n\n    # epoch_loss = tr_loss/nb_tr_steps\n    # epoch_accu = (n_correct*100)/nb_tr_examples\n    # print(f\"Validation Loss Epoch: {epoch_loss}\")\n    # print(f\"Validation Accuracy Epoch: {epoch_accu}\")\n    f1_sc_val =f1_score(targets_list,preds_list,average='micro')\n    \n    return f1_sc_val\n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T13:41:16.413384Z","iopub.execute_input":"2022-11-27T13:41:16.414109Z","iopub.status.idle":"2022-11-27T13:41:16.425262Z","shell.execute_reply.started":"2022-11-27T13:41:16.414061Z","shell.execute_reply":"2022-11-27T13:41:16.423713Z"},"trusted":true},"execution_count":28,"outputs":[]},{"cell_type":"code","source":"val_ls = []\nfor i in range(5):\n    \n    acc = valid(model, validation_loader)\n    val_ls.append(acc)\n    \nfinal_f1 = sum(val_ls)/len(val_ls)\nprint(\"F1 on test data = %0.2f%%\" % acc)","metadata":{"execution":{"iopub.status.busy":"2022-11-27T13:41:16.431138Z","iopub.execute_input":"2022-11-27T13:41:16.431702Z","iopub.status.idle":"2022-11-27T13:42:02.791609Z","shell.execute_reply.started":"2022-11-27T13:41:16.431671Z","shell.execute_reply":"2022-11-27T13:42:02.790291Z"},"trusted":true},"execution_count":29,"outputs":[{"name":"stderr","text":"242it [00:08, 27.01it/s]\n242it [00:08, 27.02it/s]\n242it [00:09, 26.21it/s]\n242it [00:08, 27.05it/s]\n242it [00:08, 27.06it/s]","output_type":"stream"},{"name":"stdout","text":"F1 on test data = 0.82%\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}]},{"cell_type":"markdown","source":"## Prediction Time","metadata":{}},{"cell_type":"code","source":"print('sahib')","metadata":{"execution":{"iopub.status.busy":"2022-11-27T12:41:01.135063Z","iopub.execute_input":"2022-11-27T12:41:01.135400Z","iopub.status.idle":"2022-11-27T12:41:01.157606Z","shell.execute_reply.started":"2022-11-27T12:41:01.135322Z","shell.execute_reply":"2022-11-27T12:41:01.156636Z"},"trusted":true},"execution_count":1,"outputs":[{"name":"stdout","text":"sahib\n","output_type":"stream"}]},{"cell_type":"code","source":"predicted_emotion_ls =[]\npredicted_class_id_ls =[]\n\nfor i in tqdm(range(len(test_df))):\n    test_text = test_df.iloc[i]['text']\n    inputs = tokenizer(test_text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        logits = model(**inputs).logits\n        predicted_class_id = logits.argmax().item()\n        predicted_emotion = emotion_map[predicted_class_id]\n        predicted_emotion_ls.append(predicted_emotion)\n        predicted_class_id_ls.append(predicted_class_id)\n        \n        \ntest_df['prediction'] = predicted_class_id_ls\ntest_df['predicted_emotion'] = predicted_emotion_ls\n \n","metadata":{"execution":{"iopub.status.busy":"2022-11-27T10:23:02.810671Z","iopub.execute_input":"2022-11-27T10:23:02.811021Z","iopub.status.idle":"2022-11-27T10:23:21.948097Z","shell.execute_reply.started":"2022-11-27T10:23:02.810992Z","shell.execute_reply":"2022-11-27T10:23:21.947006Z"},"trusted":true},"execution_count":31,"outputs":[{"name":"stderr","text":"100%|██████████| 1942/1942 [00:19<00:00, 101.56it/s]\n","output_type":"stream"}]},{"cell_type":"code","source":"test_df['true_emotion'] = test_df['labels'].map(emotion_map)\ntest_df","metadata":{"execution":{"iopub.status.busy":"2022-11-27T10:23:26.477376Z","iopub.execute_input":"2022-11-27T10:23:26.478442Z","iopub.status.idle":"2022-11-27T10:23:26.503773Z","shell.execute_reply.started":"2022-11-27T10:23:26.478388Z","shell.execute_reply":"2022-11-27T10:23:26.502811Z"},"trusted":true},"execution_count":32,"outputs":[{"execution_count":32,"output_type":"execute_result","data":{"text/plain":"                                                   text  labels  prediction  \\\n0     ਮੈਂ ਮਹਿਸੂਸ ਕਰਦਾ ਹਾਂ ਕਿ ਉਹ ਇਸ ਦੇਸ਼ ਦੇ ਨਾਗਰਿਕਾਂ ...       1           1   \n1     ਮੈਂ ਅਜੇ ਨਹੀਂ ਕਹਾਂਗਾ ਅਤੇ ਮੈਂ ਆਪਣੇ ਆਪ ਨੂੰ ਉੱਤਮ ਮ...       1           1   \n2     ਮੈਂ ਕਿਸੇ ਵੀ ਵਿਅਕਤੀ ਲਈ ਬਹੁਤ ਕੋਮਲ ਮਹਿਸੂਸ ਕਰਦਾ ਹਾ...       2           2   \n3     ਮੈਂ ਉਨ੍ਹਾਂ ਨੂੰ ਗਲੇ ਲਗਾਉਣ ਲਈ ਬੈਠ ਗਿਆ ਅਤੇ ਮਹਿਸੂਸ...       0           0   \n4          ਮੈਂ ਆਪਣੇ ਆਪ ਤੋਂ ਬਹੁਤ ਅਪਮਾਨਿਤ ਮਹਿਸੂਸ ਕਰਦਾ ਹਾਂ       0           3   \n...                                                 ...     ...         ...   \n1937  ਮੈਂ ਸਿਨਾਈ ਦੇ ਨਫ਼ਰਤ ਵਾਲੇ ਇਜ਼ਰਾਈਲੀ ਕਬਜ਼ੇ ਲਈ ਇੱਕ ...       5           5   \n1938  ਜਦੋਂ ਘਰ ਸਾਫ਼ ਹੁੰਦਾ ਹੈ ਤਾਂ ਖਾਣਾ ਵਧੀਆ ਹੁੰਦਾ ਹੈ ਅ...       0           0   \n1939  ਮੈਂ ਮਹਿਸੂਸ ਕੀਤਾ ਕਿ ਮੇਰਾ ਜਨਮਦਿਨ ਖਾਸ ਮਹਿਸੂਸ ਕਰਨ ...       1           1   \n1940                    ਮੈਂ ਦੁਬਾਰਾ ਉਦਾਸ ਮਹਿਸੂਸ ਕਰਦਾ ਹਾਂ       0           0   \n1941  ਮੈਂ ਸ਼ਨੀਵਾਰ ਦੀ ਸਵੇਰ ਨੂੰ ਕੱਚੇ ਵਾਂਗ ਮਹਿਸੂਸ ਕੀਤਾ ...       1           1   \n\n     predicted_emotion true_emotion  \n0                  joy          joy  \n1                  joy          joy  \n2                 love         love  \n3              sadness      sadness  \n4                anger      sadness  \n...                ...          ...  \n1937          surprise     surprise  \n1938           sadness      sadness  \n1939               joy          joy  \n1940           sadness      sadness  \n1941               joy          joy  \n\n[1942 rows x 5 columns]","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>text</th>\n      <th>labels</th>\n      <th>prediction</th>\n      <th>predicted_emotion</th>\n      <th>true_emotion</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>ਮੈਂ ਮਹਿਸੂਸ ਕਰਦਾ ਹਾਂ ਕਿ ਉਹ ਇਸ ਦੇਸ਼ ਦੇ ਨਾਗਰਿਕਾਂ ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>joy</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>ਮੈਂ ਅਜੇ ਨਹੀਂ ਕਹਾਂਗਾ ਅਤੇ ਮੈਂ ਆਪਣੇ ਆਪ ਨੂੰ ਉੱਤਮ ਮ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>joy</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>ਮੈਂ ਕਿਸੇ ਵੀ ਵਿਅਕਤੀ ਲਈ ਬਹੁਤ ਕੋਮਲ ਮਹਿਸੂਸ ਕਰਦਾ ਹਾ...</td>\n      <td>2</td>\n      <td>2</td>\n      <td>love</td>\n      <td>love</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>ਮੈਂ ਉਨ੍ਹਾਂ ਨੂੰ ਗਲੇ ਲਗਾਉਣ ਲਈ ਬੈਠ ਗਿਆ ਅਤੇ ਮਹਿਸੂਸ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>sadness</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>ਮੈਂ ਆਪਣੇ ਆਪ ਤੋਂ ਬਹੁਤ ਅਪਮਾਨਿਤ ਮਹਿਸੂਸ ਕਰਦਾ ਹਾਂ</td>\n      <td>0</td>\n      <td>3</td>\n      <td>anger</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>...</th>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n      <td>...</td>\n    </tr>\n    <tr>\n      <th>1937</th>\n      <td>ਮੈਂ ਸਿਨਾਈ ਦੇ ਨਫ਼ਰਤ ਵਾਲੇ ਇਜ਼ਰਾਈਲੀ ਕਬਜ਼ੇ ਲਈ ਇੱਕ ...</td>\n      <td>5</td>\n      <td>5</td>\n      <td>surprise</td>\n      <td>surprise</td>\n    </tr>\n    <tr>\n      <th>1938</th>\n      <td>ਜਦੋਂ ਘਰ ਸਾਫ਼ ਹੁੰਦਾ ਹੈ ਤਾਂ ਖਾਣਾ ਵਧੀਆ ਹੁੰਦਾ ਹੈ ਅ...</td>\n      <td>0</td>\n      <td>0</td>\n      <td>sadness</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1939</th>\n      <td>ਮੈਂ ਮਹਿਸੂਸ ਕੀਤਾ ਕਿ ਮੇਰਾ ਜਨਮਦਿਨ ਖਾਸ ਮਹਿਸੂਸ ਕਰਨ ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>joy</td>\n      <td>joy</td>\n    </tr>\n    <tr>\n      <th>1940</th>\n      <td>ਮੈਂ ਦੁਬਾਰਾ ਉਦਾਸ ਮਹਿਸੂਸ ਕਰਦਾ ਹਾਂ</td>\n      <td>0</td>\n      <td>0</td>\n      <td>sadness</td>\n      <td>sadness</td>\n    </tr>\n    <tr>\n      <th>1941</th>\n      <td>ਮੈਂ ਸ਼ਨੀਵਾਰ ਦੀ ਸਵੇਰ ਨੂੰ ਕੱਚੇ ਵਾਂਗ ਮਹਿਸੂਸ ਕੀਤਾ ...</td>\n      <td>1</td>\n      <td>1</td>\n      <td>joy</td>\n      <td>joy</td>\n    </tr>\n  </tbody>\n</table>\n<p>1942 rows × 5 columns</p>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"\ninputs = tokenizer(\"ਮੈਂ ਚੰਗਾ ਨਹੀਂ ਅੱਜ ਹਰ ਕੋਈ ਮੈਨੂੰ ਬੁਰਾ ਮਹਿਸੂਸ ਕਰਾ ਰਿਹਾ ਹੈ\", return_tensors=\"pt\").to(device)\nwith torch.no_grad():\n    logits = model(**inputs).logits\npredicted_class_id = logits.argmax().item()\nemotion_map[predicted_class_id]","metadata":{"execution":{"iopub.status.busy":"2022-11-25T11:06:27.160093Z","iopub.execute_input":"2022-11-25T11:06:27.160484Z","iopub.status.idle":"2022-11-25T11:06:27.187159Z","shell.execute_reply.started":"2022-11-25T11:06:27.160449Z","shell.execute_reply":"2022-11-25T11:06:27.186262Z"},"trusted":true},"execution_count":36,"outputs":[{"execution_count":36,"output_type":"execute_result","data":{"text/plain":"'sadness'"},"metadata":{}}]},{"cell_type":"markdown","source":"## Saved Model","metadata":{}},{"cell_type":"code","source":"output_model_file = 'pytorch_roberta_sentiment.bin'\noutput_vocab_file = './'\n\nmodel_to_save = model\ntorch.save(model_to_save, output_model_file)\ntokenizer.save_vocabulary(output_vocab_file)\n\nprint('All files saved')\nprint('This tutorial is completed')","metadata":{"execution":{"iopub.status.busy":"2022-08-26T10:04:10.376022Z","iopub.execute_input":"2022-08-26T10:04:10.376980Z","iopub.status.idle":"2022-08-26T10:04:13.105740Z","shell.execute_reply.started":"2022-08-26T10:04:10.376943Z","shell.execute_reply":"2022-08-26T10:04:13.104679Z"},"trusted":true},"execution_count":24,"outputs":[{"name":"stdout","text":"All files saved\nThis tutorial is completed\n","output_type":"stream"}]},{"cell_type":"code","source":"test.to_csv('./test_output.csv')","metadata":{"execution":{"iopub.status.busy":"2022-08-26T10:05:33.743196Z","iopub.execute_input":"2022-08-26T10:05:33.743555Z","iopub.status.idle":"2022-08-26T10:05:33.763844Z","shell.execute_reply.started":"2022-08-26T10:05:33.743524Z","shell.execute_reply":"2022-08-26T10:05:33.763056Z"},"trusted":true},"execution_count":25,"outputs":[]},{"cell_type":"raw","source":"","metadata":{}},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]}]}